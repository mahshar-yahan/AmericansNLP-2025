{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11060893,"sourceType":"datasetVersion","datasetId":6891689}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-17T23:48:47.550725Z","iopub.execute_input":"2025-03-17T23:48:47.551165Z","iopub.status.idle":"2025-03-17T23:48:47.936645Z","shell.execute_reply.started":"2025-03-17T23:48:47.551125Z","shell.execute_reply":"2025-03-17T23:48:47.935638Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/american-nlp-task-2-dataset/maya-test.tsv\n/kaggle/input/american-nlp-task-2-dataset/bribri-dev.tsv\n/kaggle/input/american-nlp-task-2-dataset/nahuatl_omitlan-test.tsv\n/kaggle/input/american-nlp-task-2-dataset/nahuatl_omitlan-dev.tsv\n/kaggle/input/american-nlp-task-2-dataset/guarani-train.tsv\n/kaggle/input/american-nlp-task-2-dataset/guarani-dev.tsv\n/kaggle/input/american-nlp-task-2-dataset/guarani-test.tsv\n/kaggle/input/american-nlp-task-2-dataset/maya-train.tsv\n/kaggle/input/american-nlp-task-2-dataset/bribri-test.tsv\n/kaggle/input/american-nlp-task-2-dataset/maya-dev.tsv\n/kaggle/input/american-nlp-task-2-dataset/nahuatl_omitlan-train.tsv\n/kaggle/input/american-nlp-task-2-dataset/bribri-train.tsv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%capture\n%pip install Dataset\n%pip install sacrebleu\n%pip install transformers\n%pip install sentencepiece\n%pip install datasets\n%pip install huggingface_hub\n%pip install bitsandbytes\n%pip install -U accelerate\n%pip install -U peft\n%pip install -U trl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T23:48:47.937967Z","iopub.execute_input":"2025-03-17T23:48:47.938468Z","iopub.status.idle":"2025-03-17T23:49:34.668444Z","shell.execute_reply.started":"2025-03-17T23:48:47.938438Z","shell.execute_reply":"2025-03-17T23:49:34.667341Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Set environment variable to help with memory allocation\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T23:49:34.670176Z","iopub.execute_input":"2025-03-17T23:49:34.670596Z","iopub.status.idle":"2025-03-17T23:49:34.674646Z","shell.execute_reply.started":"2025-03-17T23:49:34.670567Z","shell.execute_reply":"2025-03-17T23:49:34.673861Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nHUGGINGFACE_TOKEN = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n\n!huggingface-cli login --token $HUGGINGFACE_TOKEN","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T23:49:34.676139Z","iopub.execute_input":"2025-03-17T23:49:34.676471Z","iopub.status.idle":"2025-03-17T23:49:35.732154Z","shell.execute_reply.started":"2025-03-17T23:49:34.676436Z","shell.execute_reply":"2025-03-17T23:49:35.731231Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nThe token `basic task` has been saved to /root/.cache/huggingface/stored_tokens\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful.\nThe current active token is: `basic task`\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nimport bitsandbytes as bnb\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom datasets import Dataset\nfrom peft import LoraConfig, PeftConfig\nfrom trl import SFTTrainer,SFTConfig\nfrom trl import setup_chat_format\nfrom transformers import (\n                          AutoTokenizer,\n                          AutoModelForCausalLM,\n                          TrainingArguments,\n                          BitsAndBytesConfig,\n                          pipeline,\n                          Trainer,\n                          DataCollatorWithPadding,\n                          logging)\nfrom sklearn.metrics import (accuracy_score,\n                             classification_report,\n                             confusion_matrix)\nfrom sklearn.model_selection import train_test_split\nfrom sacrebleu import corpus_bleu, corpus_chrf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T23:49:35.733319Z","iopub.execute_input":"2025-03-17T23:49:35.733703Z","iopub.status.idle":"2025-03-17T23:50:02.335684Z","shell.execute_reply.started":"2025-03-17T23:49:35.733665Z","shell.execute_reply":"2025-03-17T23:50:02.335038Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from accelerate import PartialState\ndevice_map={\"\": PartialState().process_index}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T23:50:02.336406Z","iopub.execute_input":"2025-03-17T23:50:02.336964Z","iopub.status.idle":"2025-03-17T23:50:02.365242Z","shell.execute_reply.started":"2025-03-17T23:50:02.336940Z","shell.execute_reply":"2025-03-17T23:50:02.364339Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Load the data\ntrain_df = pd.read_table('/kaggle/input/american-nlp-task-2-dataset/maya-train.tsv')\ndev_df = pd.read_table('/kaggle/input/american-nlp-task-2-dataset/maya-dev.tsv')\ntest_df = pd.read_table('/kaggle/input/american-nlp-task-2-dataset/maya-test.tsv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T23:50:02.366151Z","iopub.execute_input":"2025-03-17T23:50:02.366390Z","iopub.status.idle":"2025-03-17T23:50:02.402237Z","shell.execute_reply.started":"2025-03-17T23:50:02.366369Z","shell.execute_reply":"2025-03-17T23:50:02.401596Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"X_train = train_df\nX_eval = dev_df\nX_test_sub = test_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T00:33:08.968350Z","iopub.execute_input":"2025-03-18T00:33:08.968656Z","iopub.status.idle":"2025-03-18T00:33:08.972425Z","shell.execute_reply.started":"2025-03-18T00:33:08.968625Z","shell.execute_reply":"2025-03-18T00:33:08.971658Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"def generate_prompt(data_point):\n    \"\"\"\n    Generate a structured training prompt for a given data point.\n    \"\"\"\n    return f\"\"\"Language: Mayan, Rewrite and change the following sentence according to given instruction.\nInstruction: {data_point[\"Change\"]}\nSource: {data_point[\"Source\"]}\nTarget: {data_point[\"Target\"]}\n\"\"\".strip()\n\ndef generate_test_prompt(data_point):\n    return f\"\"\"\nLanguage: Mayan, Rewrite and change the following sentence according to given instruction.\nInstruction: {data_point[\"Change\"]}\nSource: {data_point[\"Source\"]}\nProvide only the Target sentence nothing else.\nTarget:\"\"\".strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T23:50:02.409629Z","iopub.execute_input":"2025-03-17T23:50:02.409915Z","iopub.status.idle":"2025-03-17T23:50:02.424236Z","shell.execute_reply.started":"2025-03-17T23:50:02.409866Z","shell.execute_reply":"2025-03-17T23:50:02.423530Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Generate prompts for training and evaluation dataa\nX_train.loc[:,'text'] = X_train.apply(generate_prompt, axis=1)\nX_eval.loc[:,'text'] = X_eval.apply(generate_prompt, axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T23:50:02.424969Z","iopub.execute_input":"2025-03-17T23:50:02.425172Z","iopub.status.idle":"2025-03-17T23:50:02.452358Z","shell.execute_reply.started":"2025-03-17T23:50:02.425155Z","shell.execute_reply":"2025-03-17T23:50:02.451701Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Convert to datasets\ntrain_data = Dataset.from_pandas(X_train[[\"text\"]])\neval_data = Dataset.from_pandas(X_eval[[\"text\"]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T23:50:02.453155Z","iopub.execute_input":"2025-03-17T23:50:02.453387Z","iopub.status.idle":"2025-03-17T23:50:02.488345Z","shell.execute_reply.started":"2025-03-17T23:50:02.453355Z","shell.execute_reply":"2025-03-17T23:50:02.487687Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Create a new DataFrame for test prompts\ntest_data = pd.DataFrame({\n    \"Change\": X_eval[\"Change\"],\n    \"Source\": X_eval[\"Source\"]\n})\n# Generate prompts for test data\nX_test = pd.DataFrame(test_data.apply(lambda row: generate_test_prompt(row), axis=1), columns=[\"text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T23:50:02.489070Z","iopub.execute_input":"2025-03-17T23:50:02.489269Z","iopub.status.idle":"2025-03-17T23:50:02.495375Z","shell.execute_reply.started":"2025-03-17T23:50:02.489251Z","shell.execute_reply":"2025-03-17T23:50:02.494706Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Set the device (GPU if available)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T23:50:02.496066Z","iopub.execute_input":"2025-03-17T23:50:02.496305Z","iopub.status.idle":"2025-03-17T23:50:02.510896Z","shell.execute_reply.started":"2025-03-17T23:50:02.496286Z","shell.execute_reply":"2025-03-17T23:50:02.510099Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T23:50:02.511615Z","iopub.execute_input":"2025-03-17T23:50:02.511878Z","iopub.status.idle":"2025-03-17T23:50:02.529047Z","shell.execute_reply.started":"2025-03-17T23:50:02.511855Z","shell.execute_reply":"2025-03-17T23:50:02.528203Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# Load the pre-trained model and tokenizer\nbase_model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=\"float16\",\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model_name,\n    device_map=\"auto\",\n    torch_dtype=\"float16\",\n    quantization_config=bnb_config, \n)\n\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\ntokenizer.pad_token_id = tokenizer.eos_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T23:50:02.529866Z","iopub.execute_input":"2025-03-17T23:50:02.530094Z","iopub.status.idle":"2025-03-17T23:50:42.750140Z","shell.execute_reply.started":"2025-03-17T23:50:02.530068Z","shell.execute_reply":"2025-03-17T23:50:42.749122Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e547a999a68b42c683135fd10ead9ed7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f965af6f3b749fa841de8e59c082e9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ea9bb192b2444b6a0f80112a643de51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f4507a863314a8a8ae9d7e1c6aeab84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9f3ed38051d4fe8bb3af622c4d75c84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93b7057c8bd04c05b26789be76c1e1d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a84b13453f7426ca3d172dcd8a36a75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07ebdefb301240a882b903f0993ca6bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"275c058b8a2643a7b7693596c66b9a03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea7252ffd2b04b4994d7eceff746c14f"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# Move the model to the GPU\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T23:50:42.751158Z","iopub.execute_input":"2025-03-17T23:50:42.751430Z","iopub.status.idle":"2025-03-17T23:50:42.766401Z","shell.execute_reply.started":"2025-03-17T23:50:42.751408Z","shell.execute_reply":"2025-03-17T23:50:42.765202Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 3072)\n    (layers): ModuleList(\n      (0-27): 28 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n          (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n          (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n)"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# # Define a custom predict function\n# def predict(test, model, tokenizer):\n#     y_pred = []\n    \n#     for i in tqdm(range(len(test))):\n#         prompt = test.iloc[i][\"text\"]\n#         inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n        \n#         # Generate text using the model directly\n#         outputs = model.generate(**inputs, max_length=100, num_beams=4, no_repeat_ngram_size=3).to(device)\n        \n#         generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True).to(device)\n#         transformed_sentence = generated_text.split(\"Target:\")[-1].strip()\n\n#         if transformed_sentence:  \n#             y_pred.append(transformed_sentence)\n#         else:\n#             y_pred.append(\"ERROR\")  # Handle empty outputs\n    \n#     return y_pred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T23:50:42.767481Z","iopub.execute_input":"2025-03-17T23:50:42.767752Z","iopub.status.idle":"2025-03-17T23:50:44.165233Z","shell.execute_reply.started":"2025-03-17T23:50:42.767730Z","shell.execute_reply":"2025-03-17T23:50:44.164248Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from tqdm import tqdm\nfrom transformers import pipeline\n\ndef clean_prediction(text):\n    \"\"\"\n    Extracts the expected transformed sentence from the generated output.\n    \"\"\"\n    text = text.strip()\n\n    # Extract text after \"Target:\" marker\n    if \"Target:\" in text:\n        text = text.split(\"Target:\")[-1].strip()\n\n    # Take only the first line to remove unwanted repetitions\n    text = text.split(\"\\n\")[0].strip()\n\n    return text\n\ndef predict(test, model, tokenizer):\n    \"\"\"\n    Generate predictions for the test dataset without using a dataset format.\n    \"\"\"\n    y_pred = []\n    \n    # Define pipeline outside loop for efficiency\n    pipe = pipeline(task=\"text-generation\", \n                    model=model, \n                    tokenizer=tokenizer, \n                    max_new_tokens=20,  # Limit length to avoid extra output\n                    temperature=0.1,  # Make output more deterministic\n                    )  \n\n    for i in tqdm(range(len(test))):\n        prompt = test.iloc[i][\"text\"]  # Use already pre-generated test prompts\n        result = pipe(prompt)\n        \n        generated_text = result[0]['generated_text']\n        transformed_sentence = clean_prediction(generated_text)\n        y_pred.append(transformed_sentence if transformed_sentence else \"ERROR\")  # Handle empty output\n\n    return y_pred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T23:50:44.166169Z","iopub.execute_input":"2025-03-17T23:50:44.166528Z","iopub.status.idle":"2025-03-17T23:50:44.179321Z","shell.execute_reply.started":"2025-03-17T23:50:44.166494Z","shell.execute_reply":"2025-03-17T23:50:44.178639Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Evaluate the model before fine-tuning\ny_pred_before_fine_tune = predict(X_test, model, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T23:50:44.180148Z","iopub.execute_input":"2025-03-17T23:50:44.180405Z","iopub.status.idle":"2025-03-17T23:52:51.026463Z","shell.execute_reply.started":"2025-03-17T23:50:44.180384Z","shell.execute_reply":"2025-03-17T23:52:51.025558Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n  7%|▋         | 10/149 [00:09<01:59,  1.16it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n100%|██████████| 149/149 [02:06<00:00,  1.17it/s]\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Evaluate the predictions\ndef evaluate(y_true, y_pred):\n    bleu = corpus_bleu(y_pred, [y_true])\n    print(f\"BLEU score: {bleu.score:.2f}\")\n\n    chrf = corpus_chrf(y_pred, [y_true])\n    print(f\"chrF score: {chrf.score:.2f}\")\n\n    for i in range(min(5, len(y_true))):\n        print(f\"\\nMain Prompt: {X_test.iloc[i]['text']}\")\n        print(f\"Expected Sentence: {y_true[i]}\")\n        print(f\"Prediction: {y_pred[i]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T23:52:51.027381Z","iopub.execute_input":"2025-03-17T23:52:51.027691Z","iopub.status.idle":"2025-03-17T23:52:51.032263Z","shell.execute_reply.started":"2025-03-17T23:52:51.027663Z","shell.execute_reply":"2025-03-17T23:52:51.031386Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Evaluate the model\ny_true = X_eval[\"Target\"]\n\n# Evaluate the model before fine-tuning\nprint(\"\\nOriginal Model Evaluation Before Fine Tuning:\")\nevaluate(y_true.tolist(), y_pred_before_fine_tune)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T23:52:51.033061Z","iopub.execute_input":"2025-03-17T23:52:51.033298Z","iopub.status.idle":"2025-03-17T23:52:51.097611Z","shell.execute_reply.started":"2025-03-17T23:52:51.033278Z","shell.execute_reply":"2025-03-17T23:52:51.096969Z"}},"outputs":[{"name":"stdout","text":"\nOriginal Model Evaluation Before Fine Tuning:\nBLEU score: 21.39\nchrF score: 62.46\n\nMain Prompt: Language: Mayan, Rewrite and change the following sentence according to given instruction.\nInstruction: PERSON:1_PL\nSource: Janalnajen tu k'íiwikil koonol\nProvide only the Target sentence nothing else.\nTarget:\nExpected Sentence: Janalnajo'on tu k'íiwikil koonol\nPrediction: Janalnajen tu k'íiwikil koonol, k'ínch'\n\nMain Prompt: Language: Mayan, Rewrite and change the following sentence according to given instruction.\nInstruction: PERSON:2_PL\nSource: Janalnajen tu k'íiwikil koonol\nProvide only the Target sentence nothing else.\nTarget:\nExpected Sentence: Janalnaje'ex tu k'íiwikil koonol\nPrediction: Janalnajen tu k'íiwikil koonol, k'ínch'\n\nMain Prompt: Language: Mayan, Rewrite and change the following sentence according to given instruction.\nInstruction: PERSON:2_SI\nSource: Janalnajen tu k'íiwikil koonol\nProvide only the Target sentence nothing else.\nTarget:\nExpected Sentence: J-Jaanech tu k'íiwikil koonol\nPrediction: Si k'íiwikil koonol Janalnajen tu.\n\nMain Prompt: Language: Mayan, Rewrite and change the following sentence according to given instruction.\nInstruction: TYPE:NEG\nSource: Táan ek bin ich kool\nProvide only the Target sentence nothing else.\nTarget:\nExpected Sentence: Ma' táan ek bin ich kooli'\nPrediction: Táan ek bin ich kool, noo.\n\nMain Prompt: Language: Mayan, Rewrite and change the following sentence according to given instruction.\nInstruction: SUBTYPE:INT\nSource: Táan ek bin ich kool\nProvide only the Target sentence nothing else.\nTarget:\nExpected Sentence: Táan wáaj ek bin ich kool\nPrediction: Táan ek bin ich kool, ich tuk ik.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Define LoRA configuration\ndef find_all_linear_names(model):\n    cls = torch.nn.Linear\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    if 'lm_head' in lora_module_names:\n        lora_module_names.remove('lm_head')\n    return list(lora_module_names)\n\nmodules = find_all_linear_names(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T23:52:51.098356Z","iopub.execute_input":"2025-03-17T23:52:51.098602Z","iopub.status.idle":"2025-03-17T23:52:51.103977Z","shell.execute_reply.started":"2025-03-17T23:52:51.098570Z","shell.execute_reply":"2025-03-17T23:52:51.103028Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=4,\n    lora_alpha=8,\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=modules\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T23:52:51.106565Z","iopub.execute_input":"2025-03-17T23:52:51.106822Z","iopub.status.idle":"2025-03-17T23:52:51.130100Z","shell.execute_reply.started":"2025-03-17T23:52:51.106776Z","shell.execute_reply":"2025-03-17T23:52:51.129236Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Set up training arguments\ntraining_arguments = SFTConfig(\n    output_dir=\"./results\",\n    num_train_epochs=5,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    save_steps=1000,\n    dataset_text_field=\"text\",\n    max_seq_length=512,\n    packing=False,\n    logging_steps=500,\n    learning_rate=2e-4,\n    weight_decay=0.01,\n    fp16=True,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"constant\",\n    report_to=\"none\",\n    eval_strategy=\"steps\",\n    eval_steps=100,  \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T23:52:51.131164Z","iopub.execute_input":"2025-03-17T23:52:51.131417Z","iopub.status.idle":"2025-03-17T23:52:51.171813Z","shell.execute_reply.started":"2025-03-17T23:52:51.131397Z","shell.execute_reply":"2025-03-17T23:52:51.171212Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Initialize the SFTTrainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_data,\n    eval_dataset=eval_data,\n    peft_config=lora_config,\n    tokenizer=tokenizer,\n    args=training_arguments,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T23:52:51.172720Z","iopub.execute_input":"2025-03-17T23:52:51.173045Z","iopub.status.idle":"2025-03-17T23:52:52.707034Z","shell.execute_reply.started":"2025-03-17T23:52:51.173006Z","shell.execute_reply":"2025-03-17T23:52:52.706120Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-25-1a90b2d52780>:2: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n  trainer = SFTTrainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Converting train dataset to ChatML:   0%|          | 0/594 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed10cd05477c42358d0e839eaab23277"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to train dataset:   0%|          | 0/594 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"458db68f250e4186a77878716421659a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/594 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b57a9533860342bc8d6fa74050a8597b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/594 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d831c12942d442eae1e54df32e15319"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Converting eval dataset to ChatML:   0%|          | 0/149 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"babcf5ca4a9944c7af76c72ec05290ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to eval dataset:   0%|          | 0/149 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9aae06f23d1140a78dd94d990b2a6f64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing eval dataset:   0%|          | 0/149 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"169d6b71bf6d40d58a486ec2493364ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating eval dataset:   0%|          | 0/149 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d25714337ffb4e3f9a4b92fa2bc4c41a"}},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"# Train the model\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T23:52:52.708045Z","iopub.execute_input":"2025-03-17T23:52:52.708393Z","iopub.status.idle":"2025-03-18T00:15:12.602709Z","shell.execute_reply.started":"2025-03-17T23:52:52.708357Z","shell.execute_reply":"2025-03-18T00:15:12.601753Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1485' max='1485' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1485/1485 22:18, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>No log</td>\n      <td>0.936942</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>No log</td>\n      <td>0.696236</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>No log</td>\n      <td>0.571768</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>No log</td>\n      <td>0.591433</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.331600</td>\n      <td>0.579851</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.331600</td>\n      <td>0.534731</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.331600</td>\n      <td>0.545886</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.331600</td>\n      <td>0.541998</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>1.331600</td>\n      <td>0.588615</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.533100</td>\n      <td>0.563707</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.533100</td>\n      <td>0.595753</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.533100</td>\n      <td>0.560749</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.533100</td>\n      <td>0.567759</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.533100</td>\n      <td>0.589297</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1485, training_loss=0.7667590003222328, metrics={'train_runtime': 1339.3835, 'train_samples_per_second': 2.217, 'train_steps_per_second': 1.109, 'total_flos': 2922677823651840.0, 'train_loss': 0.7667590003222328})"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"# Evaluate the model after fine-tuning\ny_pred_after_fine_tune = predict(X_test, model, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T00:55:49.407935Z","iopub.execute_input":"2025-03-18T00:55:49.408254Z","iopub.status.idle":"2025-03-18T01:00:03.710302Z","shell.execute_reply.started":"2025-03-18T00:55:49.408231Z","shell.execute_reply":"2025-03-18T01:00:03.709314Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n100%|██████████| 149/149 [04:14<00:00,  1.71s/it]\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"# Evaluate the model\ny_true = X_eval[\"Target\"]\n\n# Evaluate the model before fine-tuning\nprint(\"\\nOriginal Model Evaluation After Fine Tuning:\")\nevaluate(y_true.tolist(), y_pred_after_fine_tune)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T01:00:03.711888Z","iopub.execute_input":"2025-03-18T01:00:03.712184Z","iopub.status.idle":"2025-03-18T01:00:03.760020Z","shell.execute_reply.started":"2025-03-18T01:00:03.712157Z","shell.execute_reply":"2025-03-18T01:00:03.759180Z"}},"outputs":[{"name":"stdout","text":"\nOriginal Model Evaluation After Fine Tuning:\nBLEU score: 57.16\nchrF score: 82.48\n\nMain Prompt: Language: Mayan, Rewrite and change the following sentence according to given instruction.\nInstruction: PERSON:1_PL\nSource: Janalnajen tu k'íiwikil koonol\nProvide only the Target sentence nothing else.\nTarget:\nExpected Sentence: Janalnajo'on tu k'íiwikil koonol\nPrediction: Janal koonol tu k'íiwikil koonol\n\nMain Prompt: Language: Mayan, Rewrite and change the following sentence according to given instruction.\nInstruction: PERSON:2_PL\nSource: Janalnajen tu k'íiwikil koonol\nProvide only the Target sentence nothing else.\nTarget:\nExpected Sentence: Janalnaje'ex tu k'íiwikil koonol\nPrediction: Janalnaje'ex tu k'íiwikil koonol\n\nMain Prompt: Language: Mayan, Rewrite and change the following sentence according to given instruction.\nInstruction: PERSON:2_SI\nSource: Janalnajen tu k'íiwikil koonol\nProvide only the Target sentence nothing else.\nTarget:\nExpected Sentence: J-Jaanech tu k'íiwikil koonol\nPrediction: Janalnajech tu k'íiwikil koonol\n\nMain Prompt: Language: Mayan, Rewrite and change the following sentence according to given instruction.\nInstruction: TYPE:NEG\nSource: Táan ek bin ich kool\nProvide only the Target sentence nothing else.\nTarget:\nExpected Sentence: Ma' táan ek bin ich kooli'\nPrediction: Ma' táan ek bin ich kooli'obuobi'obuobi'ob\n\nMain Prompt: Language: Mayan, Rewrite and change the following sentence according to given instruction.\nInstruction: SUBTYPE:INT\nSource: Táan ek bin ich kool\nProvide only the Target sentence nothing else.\nTarget:\nExpected Sentence: Táan wáaj ek bin ich kool\nPrediction: Táan wáaj ek bin ich kool wáaj bix ich kooli\n","output_type":"stream"}],"execution_count":49},{"cell_type":"markdown","source":"## Dev Submission","metadata":{}},{"cell_type":"code","source":"dev_pd = pd.DataFrame(y_pred_after_fine_tune, columns=['Values'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T01:00:12.266813Z","iopub.execute_input":"2025-03-18T01:00:12.267127Z","iopub.status.idle":"2025-03-18T01:00:12.271504Z","shell.execute_reply.started":"2025-03-18T01:00:12.267103Z","shell.execute_reply":"2025-03-18T01:00:12.270557Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"dev_pd.to_csv('syntax_squad_maya_dev_output.csv', sep='\\t', index=False, header=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T01:00:15.110044Z","iopub.execute_input":"2025-03-18T01:00:15.110391Z","iopub.status.idle":"2025-03-18T01:00:15.115804Z","shell.execute_reply.started":"2025-03-18T01:00:15.110354Z","shell.execute_reply":"2025-03-18T01:00:15.114869Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"from IPython.display import FileLink\n\nFileLink(\"syntax_squad_maya_dev_output.tsv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T01:00:17.041088Z","iopub.execute_input":"2025-03-18T01:00:17.041418Z","iopub.status.idle":"2025-03-18T01:00:17.047202Z","shell.execute_reply.started":"2025-03-18T01:00:17.041392Z","shell.execute_reply":"2025-03-18T01:00:17.046124Z"}},"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/syntax_squad_maya_dev_output.tsv","text/html":"<a href='syntax_squad_maya_dev_output.tsv' target='_blank'>syntax_squad_maya_dev_output.tsv</a><br>"},"metadata":{}}],"execution_count":54},{"cell_type":"markdown","source":"## Test Submission","metadata":{}},{"cell_type":"code","source":"# Create a new DataFrame for test prompts\ntest_data_sub = pd.DataFrame({\n    \"Change\": X_test_sub[\"Change\"],\n    \"Source\": X_test_sub[\"Source\"]\n})\n# Generate prompts for test data\nX_test_sub = pd.DataFrame(test_data_sub.apply(lambda row: generate_test_prompt(row), axis=1), columns=[\"text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T00:35:26.562093Z","iopub.execute_input":"2025-03-18T00:35:26.562430Z","iopub.status.idle":"2025-03-18T00:35:26.570417Z","shell.execute_reply.started":"2025-03-18T00:35:26.562407Z","shell.execute_reply":"2025-03-18T00:35:26.569657Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# Evaluate the model before fine-tuning\ny_pred_test = predict(X_test_sub, model, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T00:45:09.456398Z","iopub.execute_input":"2025-03-18T00:45:09.456624Z","iopub.status.idle":"2025-03-18T00:54:39.700227Z","shell.execute_reply.started":"2025-03-18T00:45:09.456604Z","shell.execute_reply":"2025-03-18T00:54:39.699299Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n100%|██████████| 310/310 [09:30<00:00,  1.84s/it]\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"test_pd = pd.DataFrame(y_pred_test, columns=['Values'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T00:55:03.399320Z","iopub.execute_input":"2025-03-18T00:55:03.399645Z","iopub.status.idle":"2025-03-18T00:55:03.404164Z","shell.execute_reply.started":"2025-03-18T00:55:03.399620Z","shell.execute_reply":"2025-03-18T00:55:03.403306Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"test_pd.to_csv('syntax_squad_maya_test_output.tsv', sep='\\t', index=False, header=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T00:55:22.459993Z","iopub.execute_input":"2025-03-18T00:55:22.460331Z","iopub.status.idle":"2025-03-18T00:55:22.466319Z","shell.execute_reply.started":"2025-03-18T00:55:22.460307Z","shell.execute_reply":"2025-03-18T00:55:22.465609Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"from IPython.display import FileLink\n\nFileLink(\"syntax_squad_maya_test_output.tsv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T00:55:29.299401Z","iopub.execute_input":"2025-03-18T00:55:29.299732Z","iopub.status.idle":"2025-03-18T00:55:29.305418Z","shell.execute_reply.started":"2025-03-18T00:55:29.299704Z","shell.execute_reply":"2025-03-18T00:55:29.304616Z"}},"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/syntax_squad_maya_test_output.tsv","text/html":"<a href='syntax_squad_maya_test_output.tsv' target='_blank'>syntax_squad_maya_test_output.tsv</a><br>"},"metadata":{}}],"execution_count":47}]}