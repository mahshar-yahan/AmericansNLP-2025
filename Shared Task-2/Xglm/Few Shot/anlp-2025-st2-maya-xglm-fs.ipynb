{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11170946,"sourceType":"datasetVersion","datasetId":6971476}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:54:40.259892Z","iopub.execute_input":"2025-03-26T08:54:40.260315Z","iopub.status.idle":"2025-03-26T08:54:40.572796Z","shell.execute_reply.started":"2025-03-26T08:54:40.260287Z","shell.execute_reply":"2025-03-26T08:54:40.572115Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/anlp-2025-st-2/maya-test.tsv\n/kaggle/input/anlp-2025-st-2/bribri-dev.tsv\n/kaggle/input/anlp-2025-st-2/nahuatl_omitlan-test.tsv\n/kaggle/input/anlp-2025-st-2/nahuatl_omitlan-dev.tsv\n/kaggle/input/anlp-2025-st-2/guarani-train.tsv\n/kaggle/input/anlp-2025-st-2/guarani-dev.tsv\n/kaggle/input/anlp-2025-st-2/guarani-test.tsv\n/kaggle/input/anlp-2025-st-2/maya-train.tsv\n/kaggle/input/anlp-2025-st-2/bribri-test.tsv\n/kaggle/input/anlp-2025-st-2/maya-dev.tsv\n/kaggle/input/anlp-2025-st-2/nahuatl_omitlan-train.tsv\n/kaggle/input/anlp-2025-st-2/bribri-train.tsv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%capture\n%pip install Dataset\n%pip install sacrebleu\n%pip install transformers\n%pip install sentencepiece\n%pip install datasets\n%pip install huggingface_hub\n%pip install bitsandbytes\n%pip install -U accelerate\n%pip install -U peft\n%pip install -U trl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:54:40.573989Z","iopub.execute_input":"2025-03-26T08:54:40.574452Z","iopub.status.idle":"2025-03-26T08:55:22.501060Z","shell.execute_reply.started":"2025-03-26T08:54:40.574416Z","shell.execute_reply":"2025-03-26T08:55:22.500129Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Set environment variable to help with memory allocation\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:55:22.502772Z","iopub.execute_input":"2025-03-26T08:55:22.503074Z","iopub.status.idle":"2025-03-26T08:55:22.506892Z","shell.execute_reply.started":"2025-03-26T08:55:22.503050Z","shell.execute_reply":"2025-03-26T08:55:22.506028Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nHUGGINGFACE_TOKEN = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n\n!huggingface-cli login --token $HUGGINGFACE_TOKEN","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:55:22.507941Z","iopub.execute_input":"2025-03-26T08:55:22.508209Z","iopub.status.idle":"2025-03-26T08:55:23.557590Z","shell.execute_reply.started":"2025-03-26T08:55:22.508190Z","shell.execute_reply":"2025-03-26T08:55:23.556717Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nThe token `basic task` has been saved to /root/.cache/huggingface/stored_tokens\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful.\nThe current active token is: `basic task`\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nimport bitsandbytes as bnb\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom datasets import Dataset\nfrom peft import LoraConfig, PeftConfig\nfrom trl import SFTTrainer,SFTConfig\nfrom trl import setup_chat_format\nfrom transformers import (\n                          AutoTokenizer,\n                          AutoModelForCausalLM,\n                          TrainingArguments,\n                          BitsAndBytesConfig,\n                          pipeline,\n                          Trainer,\n                          DataCollatorWithPadding,\n                          logging)\nfrom sklearn.metrics import (accuracy_score,\n                             classification_report,\n                             confusion_matrix)\nfrom sklearn.model_selection import train_test_split\nfrom sacrebleu import corpus_bleu, corpus_chrf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:55:23.558659Z","iopub.execute_input":"2025-03-26T08:55:23.558948Z","iopub.status.idle":"2025-03-26T08:55:47.767835Z","shell.execute_reply.started":"2025-03-26T08:55:23.558923Z","shell.execute_reply":"2025-03-26T08:55:47.766855Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from accelerate import PartialState\ndevice_map={\"\": PartialState().process_index}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:55:47.768998Z","iopub.execute_input":"2025-03-26T08:55:47.769666Z","iopub.status.idle":"2025-03-26T08:55:47.799490Z","shell.execute_reply.started":"2025-03-26T08:55:47.769640Z","shell.execute_reply":"2025-03-26T08:55:47.798581Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Load the data\ntrain_df = pd.read_table('/kaggle/input/anlp-2025-st-2/maya-train.tsv')\ndev_df = pd.read_table('/kaggle/input/anlp-2025-st-2/maya-dev.tsv')\ntest_df = pd.read_table('/kaggle/input/anlp-2025-st-2/maya-test.tsv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:55:47.800406Z","iopub.execute_input":"2025-03-26T08:55:47.800692Z","iopub.status.idle":"2025-03-26T08:55:47.832994Z","shell.execute_reply.started":"2025-03-26T08:55:47.800665Z","shell.execute_reply":"2025-03-26T08:55:47.832418Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"X_train = train_df\nX_eval = dev_df\nX_test_sub = test_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:55:47.833945Z","iopub.execute_input":"2025-03-26T08:55:47.834161Z","iopub.status.idle":"2025-03-26T08:55:47.837378Z","shell.execute_reply.started":"2025-03-26T08:55:47.834143Z","shell.execute_reply":"2025-03-26T08:55:47.836829Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# new type \ndef translate_tags_to_instruction(change):\n    \"\"\"\n    Translate the tags in the 'Change' field to full-form instructions, combining multiple instructions with 'and.'\n    \"\"\"\n    # Split the \"Change\" field into tags\n    instruction_tags = change.split(\", \")\n    instructions = []\n\n    # Define mapping dictionaries for each category\n    type_map = {\n        \"NEG\": \"Make the sentence negative\",\n        \"IMP\": \"Change the sentence to imperative mood\",\n        \"AFF\": \"Make the sentence affirmative\"\n    }\n\n    mood_map = {\n        \"DES\": \"Express desire or wish to perform the action\",\n        \"EXH\": \"Change to exhortative mood (encouraging or urging action)\",\n        \"ADVERS\": \"Express that the action was done despite difficulties\",\n        \"POT\": \"Express potential or ability to perform the action\",\n        \"COND\": \"Change to conditional mood\",\n        \"OPT\": \"Change to optative mood (expressing wish or hope)\",\n        \"INT\": \"Change to interrogative mood\",\n        \"NA\": \"Remove mood marking\",\n        \"IMP\": \"Change the sentence to imperative mood\"\n    }\n\n    tense_map = {\n        \"IPFV_HAB\": \"Change to habitual imperfective aspect\",\n        \"IPFV_REC\": \"Change to recent imperfective aspect\",\n        \"IPFV_PROG\": \"Change to progressive imperfective aspect\",\n        \"PRF_PROG\": \"Change to perfect progressive aspect\",\n        \"PRF_REC\": \"Change to recent perfect tense\",\n        \"FUT_POT\": \"Change to potential future tense\",\n        \"FUT_CER\": \"Change to certain future tense\",\n        \"PAS_PLU\": \"Change to pluperfect (past perfect) tense\",\n        \"PRE_SIM\": \"Change to present simple tense\",\n        \"PAS_SIM\": \"Change to past simple tense\",\n        \"FUT_SIM\": \"Change to future simple tense\"\n    }\n\n    aspect_map = {\n        \"IPFV\": \"Change to imperfective aspect\",\n        \"PFV\": \"Change to perfective aspect\",\n        \"INC\": \"Express the beginning or initiation of the action\",\n        \"DUR\": \"Express duration of the action\"\n    }\n\n    voice_map = {\n        \"MID\": \"Change to middle voice\"\n    }\n\n    absnum_map = {\n        \"PL\": \"Make the absolutive argument plural\",\n        \"NI\": \"Remove number marking from the absolutive argument\"\n    }\n\n    person_map = {\n        \"1_PL_EXC\": \"Change subject to first person plural exclusive\",\n        \"1_PL_INC\": \"Change subject to first person plural inclusive\",\n        \"2_PL\": \"Change subject to second person plural\",\n        \"3_PL\": \"Change subject to third person plural\",\n        \"2_SI\": \"Change subject to second person singular\",\n        \"3_SI\": \"Change subject to third person singular\",\n        \"1_SI\": \"Change subject to first person singular\",\n        \"1_PL\": \"Change subject to first person plural\"\n    }\n\n    poss_map = {\n        \"1_SI\": \"Change possessor to first person singular\",\n        \"1_PL\": \"Change possessor to first person plural\",\n        \"2_SI\": \"Change possessor to second person singular\",\n        \"2_PL\": \"Change possessor to second person plural\",\n        \"3_SI\": \"Change possessor to third person singular\",\n        \"3_PL\": \"Change possessor to third person plural\"\n    }\n\n    obj_map = {\n        \"1_SI\": \"Change object to first person singular\",\n        \"1_PL\": \"Change object to first person plural\",\n        \"2_SI\": \"Change object to second person singular\",\n        \"2_PL\": \"Change object to second person plural\",\n        \"3_SI\": \"Change object to third person singular\",\n        \"3_PL\": \"Change object to third person plural\"\n    }\n\n    iobj_map = {\n        \"1_SI\": \"Change indirect object to first person singular\",\n        \"1_PL\": \"Change indirect object to first person plural\",\n        \"2_SI\": \"Change indirect object to second person singular\",\n        \"2_PL\": \"Change indirect object to second person plural\",\n        \"3_SI\": \"Change indirect object to third person singular\",\n        \"3_PL\": \"Change indirect object to third person plural\"\n    }\n\n    honorific_map = {\n        \"HON:1\": \"Use honorific form\",\n        \"HON:NA\": \"Do not use honorific form\"\n    }\n\n    purposive_map = {\n        \"PURPOSIVE:VEN\": \"Express purpose or goal\",\n        \"PURPOSIVE:VET\": \"Express purpose or goal (alternative form)\",\n        \"PURPOSIVE:NA\": \"No purpose or goal indicated\"\n    }\n\n    transitivity_map = {\n        \"TRANSITIV:ITR\": \"Change to intransitive voice\"\n    }\n\n    # Translate each tag into a full-form instruction\n    for tag in instruction_tags:\n        if \":\" in tag:\n            category, value = tag.split(\":\")\n            if value != \"NA\":  # Ignore NA values\n                if category == \"TYPE\":\n                    instructions.append(type_map.get(value.strip(), f\"{category}: {value.strip()}\"))\n                elif category == \"MOOD\":\n                    instructions.append(mood_map.get(value.strip(), f\"{category}: {value.strip()}\"))\n                elif category == \"TENSE\":\n                    instructions.append(tense_map.get(value.strip(), f\"{category}: {value.strip()}\"))\n                elif category == \"ASPECT\":\n                    instructions.append(aspect_map.get(value.strip(), f\"{category}: {value.strip()}\"))\n                elif category == \"VOICE\":\n                    instructions.append(voice_map.get(value.strip(), f\"{category}: {value.strip()}\"))\n                elif category == \"ABSNUM\":\n                    instructions.append(absnum_map.get(value.strip(), f\"{category}: {value.strip()}\"))\n                elif category == \"PERSON[SUBJ]\":\n                    instructions.append(person_map.get(value.strip(), f\"{category}: {value.strip()}\"))\n                elif category == \"PERSON[POSS]\":\n                    instructions.append(poss_map.get(value.strip(), f\"{category}: {value.strip()}\"))\n                elif category == \"PERSON[OBJ]\":\n                    instructions.append(obj_map.get(value.strip(), f\"{category}: {value.strip()}\"))\n                elif category == \"PERSON[IOBJ]\":\n                    instructions.append(iobj_map.get(value.strip(), f\"{category}: {value.strip()}\"))\n                elif category == \"HON\":\n                    instructions.append(honorific_map.get(tag.strip(), f\"{category}: {value.strip()}\"))\n                elif category == \"PURPOSIVE\":\n                    instructions.append(purposive_map.get(tag.strip(), f\"{category}: {value.strip()}\"))\n                elif category == \"TRANSITIV\":\n                    instructions.append(transitivity_map.get(tag.strip(), f\"{category}: {value.strip()}\"))\n                else:\n                    instructions.append(f\"{category}: {value.strip()}\")\n        else:\n            # Handle tags without a colon\n            if tag in type_map:\n                instructions.append(type_map[tag])\n            elif tag in mood_map:\n                instructions.append(mood_map[tag])\n            elif tag in tense_map:\n                instructions.append(tense_map[tag])\n            elif tag in aspect_map:\n                instructions.append(aspect_map[tag])\n            elif tag in voice_map:\n                instructions.append(voice_map[tag])\n            elif tag in absnum_map:\n                instructions.append(absnum_map[tag])\n            else:\n                instructions.append(tag)\n\n    # Combine all instructions with 'and'\n    return ' and '.join(instructions)\n    \n# # Apply the function to the 'Change' column in both datasets\n# train_df['Instructions'] = train_df['Change'].apply(translate_tags_to_instruction)\n# dev_df['Instructions'] = dev_df['Change'].apply(translate_tags_to_instruction)\n\n# # Count the number of unknown instructions in both datasets\n# unknown_train = train_df['Instructions'].str.contains('Unknown').sum()\n# unknown_dev = dev_df['Instructions'].str.contains('Unknown').sum()\n\n# print(f\"Number of unknown instructions in training dataset: {unknown_train}\")\n# print(f\"Number of unknown instructions in development dataset: {unknown_dev}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:55:47.840141Z","iopub.execute_input":"2025-03-26T08:55:47.840387Z","iopub.status.idle":"2025-03-26T08:55:47.854836Z","shell.execute_reply.started":"2025-03-26T08:55:47.840369Z","shell.execute_reply":"2025-03-26T08:55:47.854147Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Define functions for generating prompts\ndef generate_prompt(row):\n    \"\"\"\n    Generate a structured training prompt for a given data point.\n    \"\"\"\n    instruction = translate_tags_to_instruction(row[\"Change\"])\n    return (\n        f\"Language: Bribri\\n\"\n        f\"Task: Transform the Source sentence into the Target sentence based on the given instruction.\\n\\n\"\n        f\"Example of Some of the transformations are:\\n\\n\"\n        f\"Example 1:\\n\"\n        f\"Instruction: Change to recent perfect tense. (TENSE:PRF_REC)\\n\"\n        f\"Source: Ye' shka'\\n\"\n        f\"Target: Ye' shké\\n\\n\"\n        f\"Example 2:\\n\"\n        f\"Instruction: Make the sentence negative. (TYPE:NEG)\\n\"\n        f\"Source: Ye' shka'\\n\"\n        f\"Target: Kë̀ ye' shkàne̠\\n\\n\"\n        f\"After providing these examples, rewrite the following sentence with the given instruction:\\n\\n\"\n        f\"Instruction: {row['Change']}\\n\"\n        f\"Source: {row['Source']}\\n\"\n        f\"Target:\"\n    )\n\ndef generate_test_prompt(row):\n    \"\"\"\n    Generate a structured test prompt for a given data point.\n    \"\"\"\n    instruction = translate_tags_to_instruction(row[\"Change\"])\n    return (\n        f\"Language: Bribri\\n\"\n        f\"Task: Transform the Source sentence into the Target sentence based on the given instruction.\\n\\n\"\n        f\"Example of Some of the transformations are:\\n\\n\"\n        f\"Example 1:\\n\"\n        f\"Instruction: Change to recent perfect tense. (TENSE:PRF_REC)\\n\"\n        f\"Source: Ye' shka'\\n\"\n        f\"Target: Ye' shké\\n\\n\"\n        f\"Example 2:\\n\"\n        f\"Instruction: Make the sentence negative. (TYPE:NEG)\\n\"\n        f\"Source: Ye' shka'\\n\"\n        f\"Target: Kë̀ ye' shkàne̠\\n\\n\"\n        f\"After providing these examples, rewrite the following sentence with the given instruction:\\n\\n\"\n        f\"Instruction: {row['Change']}\\n\"\n        f\"Source: {row['Source']}\\n\"\n        f\"Provide only the transformed Target sentence.\"\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:55:47.856235Z","iopub.execute_input":"2025-03-26T08:55:47.856440Z","iopub.status.idle":"2025-03-26T08:55:47.872766Z","shell.execute_reply.started":"2025-03-26T08:55:47.856423Z","shell.execute_reply":"2025-03-26T08:55:47.872173Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Generate prompts for training and evaluation dataa\nX_train.loc[:,'text'] = X_train.apply(generate_prompt, axis=1)\nX_eval.loc[:,'text'] = X_eval.apply(generate_prompt, axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:55:47.873604Z","iopub.execute_input":"2025-03-26T08:55:47.873853Z","iopub.status.idle":"2025-03-26T08:55:47.906255Z","shell.execute_reply.started":"2025-03-26T08:55:47.873829Z","shell.execute_reply":"2025-03-26T08:55:47.905437Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Convert to datasets\ntrain_data = Dataset.from_pandas(X_train[[\"text\"]])\neval_data = Dataset.from_pandas(X_eval[[\"text\"]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:55:47.907077Z","iopub.execute_input":"2025-03-26T08:55:47.907272Z","iopub.status.idle":"2025-03-26T08:55:47.942465Z","shell.execute_reply.started":"2025-03-26T08:55:47.907255Z","shell.execute_reply":"2025-03-26T08:55:47.941851Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Create a new DataFrame for test prompts\ntest_data = pd.DataFrame({\n    \"Change\": X_eval[\"Change\"],\n    \"Source\": X_eval[\"Source\"]\n})\n# Generate prompts for test data\nX_test = pd.DataFrame(test_data.apply(lambda row: generate_test_prompt(row), axis=1), columns=[\"text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:55:47.943168Z","iopub.execute_input":"2025-03-26T08:55:47.943395Z","iopub.status.idle":"2025-03-26T08:55:47.950157Z","shell.execute_reply.started":"2025-03-26T08:55:47.943364Z","shell.execute_reply":"2025-03-26T08:55:47.949402Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Set the device (GPU if available)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:55:47.950943Z","iopub.execute_input":"2025-03-26T08:55:47.951188Z","iopub.status.idle":"2025-03-26T08:55:47.963184Z","shell.execute_reply.started":"2025-03-26T08:55:47.951159Z","shell.execute_reply":"2025-03-26T08:55:47.962383Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:55:47.964021Z","iopub.execute_input":"2025-03-26T08:55:47.964293Z","iopub.status.idle":"2025-03-26T08:55:47.978476Z","shell.execute_reply.started":"2025-03-26T08:55:47.964267Z","shell.execute_reply":"2025-03-26T08:55:47.977853Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# Load the pre-trained model and tokenizer\nbase_model_name = \"facebook/xglm-1.7B\"\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=\"float16\",\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model_name,\n    device_map=\"auto\",\n    torch_dtype=\"float16\",\n    quantization_config=bnb_config, \n)\n\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\ntokenizer.pad_token_id = tokenizer.eos_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:55:47.979321Z","iopub.execute_input":"2025-03-26T08:55:47.979519Z","iopub.status.idle":"2025-03-26T08:56:09.452727Z","shell.execute_reply.started":"2025-03-26T08:55:47.979502Z","shell.execute_reply":"2025-03-26T08:56:09.451883Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/548 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79779cea67e146448ed42eec9cb0d4fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/3.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4488ee3a0e23409d9f5955516b205ec6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c353a58f22d94c0c9d5936f0f6dc334b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/335 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43965be2edf84434bdd6af59f23ab134"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/4.92M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c8743094b084585b160a765b99efc76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59bcb58f70384554a4b07dd4fb7cf25c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/276 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a14f03c2c71c47dd814aab9d54e9afd0"}},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"# Move the model to the GPU\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:56:09.453865Z","iopub.execute_input":"2025-03-26T08:56:09.454112Z","iopub.status.idle":"2025-03-26T08:56:09.468509Z","shell.execute_reply.started":"2025-03-26T08:56:09.454092Z","shell.execute_reply":"2025-03-26T08:56:09.467907Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"XGLMForCausalLM(\n  (model): XGLMModel(\n    (embed_tokens): XGLMScaledWordEmbedding(256008, 2048, padding_idx=1)\n    (embed_positions): XGLMSinusoidalPositionalEmbedding()\n    (layers): ModuleList(\n      (0-23): 24 x XGLMDecoderLayer(\n        (self_attn): XGLMAttention(\n          (k_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n          (v_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n          (out_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n        )\n        (activation_fn): GELUActivation()\n        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n        (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n        (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=2048, out_features=256008, bias=False)\n)"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# # Define a custom predict function\n# def predict(test, model, tokenizer):\n#     y_pred = []\n    \n#     for i in tqdm(range(len(test))):\n#         prompt = test.iloc[i][\"text\"]\n#         inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n        \n#         # Generate text using the model directly\n#         outputs = model.generate(**inputs, max_length=100, num_beams=4, no_repeat_ngram_size=3).to(device)\n        \n#         generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True).to(device)\n#         transformed_sentence = generated_text.split(\"Target:\")[-1].strip()\n\n#         if transformed_sentence:  \n#             y_pred.append(transformed_sentence)\n#         else:\n#             y_pred.append(\"ERROR\")  # Handle empty outputs\n    \n#     return y_pred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:56:09.469416Z","iopub.execute_input":"2025-03-26T08:56:09.469652Z","iopub.status.idle":"2025-03-26T08:56:10.631161Z","shell.execute_reply.started":"2025-03-26T08:56:09.469634Z","shell.execute_reply":"2025-03-26T08:56:10.630185Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from tqdm import tqdm\nfrom transformers import pipeline\n\ndef clean_prediction(text):\n    \"\"\"\n    Extracts the expected transformed sentence from the generated output.\n    \"\"\"\n    text = text.strip()\n\n    # Extract text after \"Target:\" marker\n    if \"Target:\" in text:\n        text = text.split(\"Target:\")[-1].strip()\n\n    # Take only the first line to remove unwanted repetitions\n    text = text.split(\"\\n\")[0].strip()\n\n    return text\n\ndef predict(test, model, tokenizer):\n    \"\"\"\n    Generate predictions for the test dataset without using a dataset format.\n    \"\"\"\n    y_pred = []\n    \n    # Define pipeline outside loop for efficiency\n    pipe = pipeline(task=\"text-generation\", \n                    model=model, \n                    tokenizer=tokenizer, \n                    max_new_tokens=20,  # Limit length to avoid extra output\n                    temperature=0.1,  # Make output more deterministic\n                    )  \n\n    for i in tqdm(range(len(test))):\n        prompt = test.iloc[i][\"text\"]  # Use already pre-generated test prompts\n        result = pipe(prompt)\n        \n        generated_text = result[0]['generated_text']\n        transformed_sentence = clean_prediction(generated_text)\n        y_pred.append(transformed_sentence if transformed_sentence else \"ERROR\")  # Handle empty output\n\n    return y_pred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:56:10.632216Z","iopub.execute_input":"2025-03-26T08:56:10.632483Z","iopub.status.idle":"2025-03-26T08:56:10.654331Z","shell.execute_reply.started":"2025-03-26T08:56:10.632460Z","shell.execute_reply":"2025-03-26T08:56:10.653469Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Evaluate the predictions\ndef evaluate(y_true, y_pred):\n    bleu = corpus_bleu(y_pred, [y_true])\n    print(f\"BLEU score: {bleu.score:.2f}\")\n\n    chrf = corpus_chrf(y_pred, [y_true])\n    print(f\"chrF score: {chrf.score:.2f}\")\n\n    # Accuracy calculation\n    accuracy = accuracy_score(y_true, y_pred)\n    print(f\"Accuracy: {accuracy:.2f}\")\n    \n    for i in range(min(5, len(y_true))):\n        print(f\"\\nMain Prompt: {X_test.iloc[i]['text']}\")\n        print(f\"Expected Sentence: {y_true[i]}\")\n        print(f\"Prediction: {y_pred[i]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:56:10.655789Z","iopub.execute_input":"2025-03-26T08:56:10.656307Z","iopub.status.idle":"2025-03-26T08:56:10.674273Z","shell.execute_reply.started":"2025-03-26T08:56:10.656273Z","shell.execute_reply":"2025-03-26T08:56:10.673365Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Define LoRA configuration\ndef find_all_linear_names(model):\n    cls = torch.nn.Linear\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    if 'lm_head' in lora_module_names:\n        lora_module_names.remove('lm_head')\n    return list(lora_module_names)\n\nmodules = find_all_linear_names(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:56:10.675298Z","iopub.execute_input":"2025-03-26T08:56:10.675617Z","iopub.status.idle":"2025-03-26T08:56:10.691006Z","shell.execute_reply.started":"2025-03-26T08:56:10.675587Z","shell.execute_reply":"2025-03-26T08:56:10.690073Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=4,\n    lora_alpha=8,\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=modules\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:56:10.691969Z","iopub.execute_input":"2025-03-26T08:56:10.692251Z","iopub.status.idle":"2025-03-26T08:56:10.710864Z","shell.execute_reply.started":"2025-03-26T08:56:10.692221Z","shell.execute_reply":"2025-03-26T08:56:10.709777Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Set up training arguments\ntraining_arguments = SFTConfig(\n    output_dir=\"./results\",\n    num_train_epochs=5,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    save_steps=1000,\n    dataset_text_field=\"text\",\n    max_seq_length=512,\n    packing=False,\n    logging_steps=500,\n    learning_rate=2e-4,\n    weight_decay=0.01,\n    fp16=True,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"constant\",\n    report_to=\"none\",\n    eval_strategy=\"steps\",\n    eval_steps=50,  \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:56:10.712052Z","iopub.execute_input":"2025-03-26T08:56:10.712368Z","iopub.status.idle":"2025-03-26T08:56:10.756917Z","shell.execute_reply.started":"2025-03-26T08:56:10.712340Z","shell.execute_reply":"2025-03-26T08:56:10.755670Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Initialize the SFTTrainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_data,\n    eval_dataset=eval_data,\n    peft_config=lora_config,\n    args=training_arguments,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:56:10.758280Z","iopub.execute_input":"2025-03-26T08:56:10.758713Z","iopub.status.idle":"2025-03-26T08:56:13.999581Z","shell.execute_reply.started":"2025-03-26T08:56:10.758678Z","shell.execute_reply":"2025-03-26T08:56:13.999010Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Converting train dataset to ChatML:   0%|          | 0/594 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20153c40b07440f29cb669a074404e0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to train dataset:   0%|          | 0/594 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b946336e73a04e50926afc406c984030"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/594 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"453708a60e14419d816af06f42272937"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/594 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66a0c2b0135f4ecaabdb0bc96755ce21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Converting eval dataset to ChatML:   0%|          | 0/149 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b9be1a8cbb84f23a0075628a66a0abe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to eval dataset:   0%|          | 0/149 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"289524b2d81f43d391f2f60c3dbb1536"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing eval dataset:   0%|          | 0/149 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf364866ac954ed49f033b32e72cafa7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating eval dataset:   0%|          | 0/149 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77e6fdfdebf34144818eff184786b8eb"}},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"# Train the model\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:56:14.000271Z","iopub.execute_input":"2025-03-26T08:56:14.000556Z","iopub.status.idle":"2025-03-26T09:22:07.958300Z","shell.execute_reply.started":"2025-03-26T08:56:14.000528Z","shell.execute_reply":"2025-03-26T09:22:07.957407Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1485' max='1485' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1485/1485 25:51, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>No log</td>\n      <td>1.102133</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>No log</td>\n      <td>0.633506</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>No log</td>\n      <td>0.528560</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>No log</td>\n      <td>0.502685</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>No log</td>\n      <td>0.515703</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>No log</td>\n      <td>0.495945</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>No log</td>\n      <td>0.531032</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>No log</td>\n      <td>0.521797</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>No log</td>\n      <td>0.494635</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.723300</td>\n      <td>0.529526</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.723300</td>\n      <td>0.528019</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.723300</td>\n      <td>0.451914</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.723300</td>\n      <td>0.483659</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.723300</td>\n      <td>0.542552</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.723300</td>\n      <td>0.521914</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.723300</td>\n      <td>0.553031</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.723300</td>\n      <td>0.609668</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.723300</td>\n      <td>0.554200</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.723300</td>\n      <td>0.576626</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.226400</td>\n      <td>0.551801</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.226400</td>\n      <td>0.578772</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.226400</td>\n      <td>0.550376</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.226400</td>\n      <td>0.549688</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.226400</td>\n      <td>0.597688</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.226400</td>\n      <td>0.612495</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.226400</td>\n      <td>0.623229</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.226400</td>\n      <td>0.622346</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.226400</td>\n      <td>0.591508</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.226400</td>\n      <td>0.647338</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1485, training_loss=0.38072651564472854, metrics={'train_runtime': 1553.371, 'train_samples_per_second': 1.912, 'train_steps_per_second': 0.956, 'total_flos': 2945685831475200.0, 'train_loss': 0.38072651564472854})"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"# Evaluate the model after fine-tuning\ny_pred_after_fine_tune = predict(X_test, model, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T09:22:07.959218Z","iopub.execute_input":"2025-03-26T09:22:07.959496Z","iopub.status.idle":"2025-03-26T09:25:22.361328Z","shell.execute_reply.started":"2025-03-26T09:22:07.959475Z","shell.execute_reply":"2025-03-26T09:25:22.360451Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n  0%|          | 0/149 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n  7%|▋         | 10/149 [00:13<03:02,  1.31s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n100%|██████████| 149/149 [03:14<00:00,  1.30s/it]\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# Evaluate the model\ny_true = X_eval[\"Target\"]\n\n# Evaluate the model before fine-tuning\nprint(\"\\nOriginal Model Evaluation After Fine Tuning:\")\nevaluate(y_true.tolist(), y_pred_after_fine_tune)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T09:25:22.364626Z","iopub.execute_input":"2025-03-26T09:25:22.364904Z","iopub.status.idle":"2025-03-26T09:25:22.396189Z","shell.execute_reply.started":"2025-03-26T09:25:22.364880Z","shell.execute_reply":"2025-03-26T09:25:22.395546Z"}},"outputs":[{"name":"stdout","text":"\nOriginal Model Evaluation After Fine Tuning:\nBLEU score: 0.00\nchrF score: 2.52\nAccuracy: 0.00\n\nMain Prompt: Language: Bribri\nTask: Transform the Source sentence into the Target sentence based on the given instruction.\n\nExample of Some of the transformations are:\n\nExample 1:\nInstruction: Change to recent perfect tense. (TENSE:PRF_REC)\nSource: Ye' shka'\nTarget: Ye' shké\n\nExample 2:\nInstruction: Make the sentence negative. (TYPE:NEG)\nSource: Ye' shka'\nTarget: Kë̀ ye' shkàne̠\n\nAfter providing these examples, rewrite the following sentence with the given instruction:\n\nInstruction: PERSON:1_PL\nSource: Janalnajen tu k'íiwikil koonol\nProvide only the transformed Target sentence.\nExpected Sentence: Janalnajo'on tu k'íiwikil koonol\nPrediction: Kë̀ ye' shkàne̠\n\nMain Prompt: Language: Bribri\nTask: Transform the Source sentence into the Target sentence based on the given instruction.\n\nExample of Some of the transformations are:\n\nExample 1:\nInstruction: Change to recent perfect tense. (TENSE:PRF_REC)\nSource: Ye' shka'\nTarget: Ye' shké\n\nExample 2:\nInstruction: Make the sentence negative. (TYPE:NEG)\nSource: Ye' shka'\nTarget: Kë̀ ye' shkàne̠\n\nAfter providing these examples, rewrite the following sentence with the given instruction:\n\nInstruction: PERSON:2_PL\nSource: Janalnajen tu k'íiwikil koonol\nProvide only the transformed Target sentence.\nExpected Sentence: Janalnaje'ex tu k'íiwikil koonol\nPrediction: ERROR\n\nMain Prompt: Language: Bribri\nTask: Transform the Source sentence into the Target sentence based on the given instruction.\n\nExample of Some of the transformations are:\n\nExample 1:\nInstruction: Change to recent perfect tense. (TENSE:PRF_REC)\nSource: Ye' shka'\nTarget: Ye' shké\n\nExample 2:\nInstruction: Make the sentence negative. (TYPE:NEG)\nSource: Ye' shka'\nTarget: Kë̀ ye' shkàne̠\n\nAfter providing these examples, rewrite the following sentence with the given instruction:\n\nInstruction: PERSON:2_SI\nSource: Janalnajen tu k'íiwikil koonol\nProvide only the transformed Target sentence.\nExpected Sentence: J-Jaanech tu k'íiwikil koonol\nPrediction: Kë̀ ye' shkàne̠\n\nMain Prompt: Language: Bribri\nTask: Transform the Source sentence into the Target sentence based on the given instruction.\n\nExample of Some of the transformations are:\n\nExample 1:\nInstruction: Change to recent perfect tense. (TENSE:PRF_REC)\nSource: Ye' shka'\nTarget: Ye' shké\n\nExample 2:\nInstruction: Make the sentence negative. (TYPE:NEG)\nSource: Ye' shka'\nTarget: Kë̀ ye' shkàne̠\n\nAfter providing these examples, rewrite the following sentence with the given instruction:\n\nInstruction: TYPE:NEG\nSource: Táan ek bin ich kool\nProvide only the transformed Target sentence.\nExpected Sentence: Ma' táan ek bin ich kooli'\nPrediction: ERROR\n\nMain Prompt: Language: Bribri\nTask: Transform the Source sentence into the Target sentence based on the given instruction.\n\nExample of Some of the transformations are:\n\nExample 1:\nInstruction: Change to recent perfect tense. (TENSE:PRF_REC)\nSource: Ye' shka'\nTarget: Ye' shké\n\nExample 2:\nInstruction: Make the sentence negative. (TYPE:NEG)\nSource: Ye' shka'\nTarget: Kë̀ ye' shkàne̠\n\nAfter providing these examples, rewrite the following sentence with the given instruction:\n\nInstruction: SUBTYPE:INT\nSource: Táan ek bin ich kool\nProvide only the transformed Target sentence.\nExpected Sentence: Táan wáaj ek bin ich kool\nPrediction: ERROR\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"## Dev Submission","metadata":{}},{"cell_type":"code","source":"dev_pd = pd.DataFrame(y_pred_after_fine_tune, columns=['Values'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T09:25:22.397420Z","iopub.execute_input":"2025-03-26T09:25:22.397704Z","iopub.status.idle":"2025-03-26T09:25:22.401679Z","shell.execute_reply.started":"2025-03-26T09:25:22.397683Z","shell.execute_reply":"2025-03-26T09:25:22.400861Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"dev_pd.to_csv('syntax_squad_bribri_dev_output.tsv', sep='\\t', index=False, header=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T09:25:22.402553Z","iopub.execute_input":"2025-03-26T09:25:22.402801Z","iopub.status.idle":"2025-03-26T09:25:22.422103Z","shell.execute_reply.started":"2025-03-26T09:25:22.402772Z","shell.execute_reply":"2025-03-26T09:25:22.421398Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"from IPython.display import FileLink\n\nFileLink(\"syntax_squad_bribri_dev_output.tsv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T09:25:22.422834Z","iopub.execute_input":"2025-03-26T09:25:22.423105Z","iopub.status.idle":"2025-03-26T09:25:22.432608Z","shell.execute_reply.started":"2025-03-26T09:25:22.423074Z","shell.execute_reply":"2025-03-26T09:25:22.431756Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/syntax_squad_bribri_dev_output.tsv","text/html":"<a href='syntax_squad_bribri_dev_output.tsv' target='_blank'>syntax_squad_bribri_dev_output.tsv</a><br>"},"metadata":{}}],"execution_count":30},{"cell_type":"markdown","source":"## Test Submission","metadata":{}},{"cell_type":"code","source":"# Create a new DataFrame for test prompts\ntest_data_sub = pd.DataFrame({\n    \"Change\": X_test_sub[\"Change\"],\n    \"Source\": X_test_sub[\"Source\"]\n})\n# Generate prompts for test data\nX_test_sub = pd.DataFrame(test_data_sub.apply(lambda row: generate_test_prompt(row), axis=1), columns=[\"text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T09:25:22.433375Z","iopub.execute_input":"2025-03-26T09:25:22.433626Z","iopub.status.idle":"2025-03-26T09:25:22.451464Z","shell.execute_reply.started":"2025-03-26T09:25:22.433596Z","shell.execute_reply":"2025-03-26T09:25:22.450833Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Evaluate the model before fine-tuning\ny_pred_test = predict(X_test_sub, model, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T09:25:22.452364Z","iopub.execute_input":"2025-03-26T09:25:22.452649Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n  0%|          | 0/310 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n 69%|██████▉   | 214/310 [04:37<02:07,  1.32s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"test_pd = pd.DataFrame(y_pred_test, columns=['Values'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_pd.to_csv('syntax_squad_bribri_test_output.tsv', sep='\\t', index=False, header=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\n\nFileLink(\"syntax_squad_bribri_test_output.tsv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}